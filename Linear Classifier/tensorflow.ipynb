{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Only need '%matplotlib inline' when running in ipython notebook.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(set_type):\n",
    "    \"\"\"Get data from files and storage them in a array. Return the data_set and label_set.\n",
    "    \n",
    "    set_type    the type of data set you want to build, including train dataset, dev dataset \n",
    "                and eval dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = {'train': 'train/lab/hw1train_labels.txt', 'dev': 'dev/lab/hw1dev_labels.txt', \\\n",
    "                 'eval': 'eval/lab/hw1eval_labels.txt'} \n",
    "\n",
    "    label_array = np.loadtxt(data_path[set_type], dtype='string')\n",
    "\n",
    "    #creat empty arrays to insert label and data\n",
    "    label_set = np.zeros([len(label_array), 2])\n",
    "    data_set = np.zeros([len(label_array), 16])\n",
    "\n",
    "    #the first column of the label file is the label,\n",
    "    #the second column is the corresbonding data file nam\n",
    "    for i in range(len(label_set)): \n",
    "        \n",
    "        #build the label set\n",
    "        if int(label_array[i][0]) == 0:\n",
    "            label_set[i][0] = 1 #insert label into label_set\n",
    "        else:\n",
    "            label_set[i][1] = 1\n",
    "            \n",
    "        #build the data set\n",
    "        with open(label_array[i][1]) as data_file:\n",
    "            data = data_file.readlines()[0].split() #find the data accoding to label\n",
    "        for j in range(len(data)):\n",
    "            data_set[i][j] = data[j] #insert data into the dataset\n",
    "            \n",
    "    data_set, label_set = nan_check(data_set, label_set) #delete the rows containing 'nan'\n",
    "    \n",
    "    return data_set, label_set #return the data set and label set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_check(data, label):\n",
    "    \"\"\"Find out the rows in datasets and delete these rows\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nan_rows = np.array(0); #define an array containg the no. of rows having 'nan'\n",
    "    \n",
    "    #collect all the numbers of 'nan'-data rows\n",
    "    for i in range(len(data)):\n",
    "        for j in range(16):\n",
    "            if str(data[i][j]) == 'nan':\n",
    "                nan_rows = np.append(nan_rows, i)\n",
    "    nan_rows = np.delete(nan_rows, 0) #delete the first element of nan_rows which was made to fit the append()\n",
    "    \n",
    "    return np.delete(data, nan_rows, 0), np.delete(label, nan_rows, 0) #output the dataset whose 'nan'-data rows have been deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(data_set, label_set):\n",
    "    \"\"\"Randomly shuffle the data and label\n",
    "    \n",
    "    data_set    the data samples\n",
    "    \n",
    "    label_set   the lables\n",
    "    \"\"\"\n",
    "    \n",
    "    shuffled_data = np.zeros((data_set.shape))\n",
    "    shuffled_label = np.zeros((label_set.shape))\n",
    "    idx = np.array(xrange(len(label_set)))\n",
    "    random.shuffle(idx)\n",
    "    i = 0\n",
    "    for j in idx:\n",
    "        shuffled_data[i] = data_set[int(j)]\n",
    "        shuffled_label[i] = label_set[int(j)]\n",
    "        i += 1\n",
    "    return shuffled_data, shuffled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8774, 16) (8774, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = get_data('train')\n",
    "train_data, train_label = shuffle(train_data, train_label)\n",
    "print train_data.shape, train_label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 1.603665\n",
      "0.648849\n",
      "Loss at step 60: 1.136071\n",
      "0.688511\n",
      "Loss at step 120: 0.817993\n",
      "0.737064\n",
      "Loss at step 180: 0.597623\n",
      "0.788694\n",
      "Loss at step 240: 0.455304\n",
      "0.831548\n",
      "Loss at step 300: 0.367535\n",
      "0.862093\n",
      "Loss at step 360: 0.314238\n",
      "0.88295\n",
      "Loss at step 420: 0.281918\n",
      "0.895943\n",
      "Loss at step 480: 0.262282\n",
      "0.903009\n",
      "Loss at step 540: 0.250210\n",
      "0.908708\n",
      "Loss at step 600: 0.242545\n",
      "0.91338\n",
      "Loss at step 660: 0.237406\n",
      "0.916116\n",
      "Loss at step 720: 0.233723\n",
      "0.918053\n",
      "Loss at step 780: 0.230909\n",
      "0.919763\n",
      "Loss at step 840: 0.228641\n",
      "0.922384\n",
      "Loss at step 900: 0.226739\n",
      "0.923068\n",
      "Loss at step 960: 0.225099\n",
      "0.92341\n",
      "Loss at step 1020: 0.223656\n",
      "0.92455\n",
      "Loss at step 1080: 0.222368\n",
      "0.92569\n",
      "Loss at step 1140: 0.221205\n",
      "0.92569\n",
      "Loss at step 1200: 0.220146\n",
      "0.926259\n",
      "Loss at step 1260: 0.219174\n",
      "0.926487\n",
      "Loss at step 1320: 0.218277\n",
      "0.926829\n",
      "Loss at step 1380: 0.217446\n",
      "0.927399\n",
      "Loss at step 1440: 0.216672\n",
      "0.927855\n",
      "Loss at step 1500: 0.215949\n",
      "0.927855\n",
      "Loss at step 1560: 0.215270\n",
      "0.927855\n",
      "Loss at step 1620: 0.214632\n",
      "0.928197\n",
      "Loss at step 1680: 0.214030\n",
      "0.928539\n",
      "Loss at step 1740: 0.213462\n",
      "0.928539\n",
      "Loss at step 1800: 0.212924\n",
      "0.928881\n",
      "Loss at step 1860: 0.212413\n",
      "0.928995\n",
      "Loss at step 1920: 0.211929\n",
      "0.929109\n",
      "Loss at step 1980: 0.211468\n",
      "0.929565\n",
      "Loss at step 2040: 0.211029\n",
      "0.929793\n",
      "Loss at step 2100: 0.210611\n",
      "0.929793\n",
      "Loss at step 2160: 0.210212\n",
      "0.929793\n",
      "Loss at step 2220: 0.209830\n",
      "0.930362\n",
      "Loss at step 2280: 0.209466\n",
      "0.930362\n",
      "Loss at step 2340: 0.209117\n",
      "0.930476\n",
      "Loss at step 2400: 0.208783\n",
      "0.930476\n",
      "Loss at step 2460: 0.208463\n",
      "0.930362\n",
      "Loss at step 2520: 0.208156\n",
      "0.930476\n",
      "Loss at step 2580: 0.207862\n",
      "0.930818\n",
      "Loss at step 2640: 0.207579\n",
      "0.930704\n",
      "Loss at step 2700: 0.207307\n",
      "0.930704\n",
      "Loss at step 2760: 0.207046\n",
      "0.93059\n",
      "Loss at step 2820: 0.206795\n",
      "0.930362\n",
      "Loss at step 2880: 0.206554\n",
      "0.930248\n",
      "Loss at step 2940: 0.206321\n",
      "0.93059\n",
      "Loss at step 3000: 0.206097\n",
      "0.930818\n",
      "Loss at step 3060: 0.205882\n",
      "0.930932\n",
      "Loss at step 3120: 0.205674\n",
      "0.930932\n",
      "Loss at step 3180: 0.205473\n",
      "0.930818\n",
      "Loss at step 3240: 0.205280\n",
      "0.930704\n",
      "Loss at step 3300: 0.205093\n",
      "0.93059\n",
      "Loss at step 3360: 0.204913\n",
      "0.93059\n",
      "Loss at step 3420: 0.204739\n",
      "0.930704\n",
      "Loss at step 3480: 0.204571\n",
      "0.930704\n",
      "Loss at step 3540: 0.204408\n",
      "0.931046\n",
      "Loss at step 3600: 0.204251\n",
      "0.93116\n",
      "Loss at step 3660: 0.204099\n",
      "0.93116\n",
      "Loss at step 3720: 0.203952\n",
      "0.93116\n",
      "Loss at step 3780: 0.203809\n",
      "0.931274\n",
      "Loss at step 3840: 0.203671\n",
      "0.931388\n",
      "Loss at step 3900: 0.203537\n",
      "0.93116\n",
      "Loss at step 3960: 0.203408\n",
      "0.931046\n",
      "Loss at step 4020: 0.203282\n",
      "0.931046\n",
      "Loss at step 4080: 0.203161\n",
      "0.930932\n",
      "Loss at step 4140: 0.203042\n",
      "0.930818\n",
      "Loss at step 4200: 0.202928\n",
      "0.930932\n",
      "Loss at step 4260: 0.202817\n",
      "0.931046\n",
      "Loss at step 4320: 0.202709\n",
      "0.931046\n",
      "Loss at step 4380: 0.202604\n",
      "0.931274\n",
      "Loss at step 4440: 0.202502\n",
      "0.931274\n",
      "Loss at step 4500: 0.202403\n",
      "0.931388\n",
      "Loss at step 4560: 0.202307\n",
      "0.931388\n",
      "Loss at step 4620: 0.202213\n",
      "0.931502\n",
      "Loss at step 4680: 0.202122\n",
      "0.931388\n",
      "Loss at step 4740: 0.202033\n",
      "0.931274\n",
      "Loss at step 4800: 0.201947\n",
      "0.93116\n",
      "Loss at step 4860: 0.201863\n",
      "0.931046\n",
      "Loss at step 4920: 0.201781\n",
      "0.93116\n",
      "Loss at step 4980: 0.201702\n",
      "0.931046\n",
      "Loss at step 5040: 0.201624\n",
      "0.930932\n",
      "Loss at step 5100: 0.201548\n",
      "0.931046\n",
      "Loss at step 5160: 0.201474\n",
      "0.931274\n",
      "Loss at step 5220: 0.201402\n",
      "0.931388\n",
      "Loss at step 5280: 0.201332\n",
      "0.931388\n",
      "Loss at step 5340: 0.201263\n",
      "0.931274\n",
      "Loss at step 5400: 0.201196\n",
      "0.931388\n",
      "Loss at step 5460: 0.201131\n",
      "0.931388\n",
      "Loss at step 5520: 0.201067\n",
      "0.931388\n",
      "Loss at step 5580: 0.201005\n",
      "0.931502\n",
      "Loss at step 5640: 0.200944\n",
      "0.931616\n",
      "Loss at step 5700: 0.200884\n",
      "0.931844\n",
      "Loss at step 5760: 0.200826\n",
      "0.931844\n",
      "Loss at step 5820: 0.200769\n",
      "0.931844\n",
      "Loss at step 5880: 0.200714\n",
      "0.931958\n",
      "Loss at step 5940: 0.200659\n",
      "0.931844\n",
      "[[ 1.58036327  1.57852268]\n",
      " [ 0.37911168  0.4251087 ]\n",
      " [ 1.67662358  0.17188677]\n",
      " [-0.24863014 -0.99292266]\n",
      " [ 0.07899719  0.27804369]\n",
      " [ 0.99968392  0.64055139]\n",
      " [-2.47207856 -0.7407198 ]\n",
      " [-1.92328942  0.64419699]\n",
      " [ 1.56329298  1.58574045]\n",
      " [ 0.60473698 -0.40495801]\n",
      " [-0.54339606 -1.31652141]\n",
      " [ 1.07540572  0.54218173]\n",
      " [-0.57068217 -1.5070796 ]\n",
      " [ 0.9724201  -0.36878282]\n",
      " [-0.65337503 -0.60803044]\n",
      " [-0.57054412  0.81349444]]\n"
     ]
    }
   ],
   "source": [
    "x_placeholder = tf.placeholder(tf.float32, [None, 16])\n",
    "y_placeholder = tf.placeholder(tf.float32, [None, 2])\n",
    "w = tf.Variable(tf.random_normal([16, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "y_hat = tf.nn.softmax(tf.matmul(x_placeholder, w) + b)\n",
    "\n",
    "#loss = tf.reduce_sum(tf.square(y_placeholder-y_hat)) / train_data.shape[0]\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_placeholder * tf.log(y_hat), reduction_indices=[1]))\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y_placeholder,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "y_error = []\n",
    "\n",
    "for step in range(6000):\n",
    "    feed_dict = {x_placeholder: train_data, y_placeholder: train_label}\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "    loss_np = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 60 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (step, loss_np))\n",
    "        print(sess.run(accuracy, feed_dict={x_placeholder: train_data, y_placeholder: train_label}))\n",
    "        y_error.append(loss_np)\n",
    "print sess.run(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x129d4a390>]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGTpJREFUeJzt3X+QHOV95/H3Rz8jGRA/BIJIlgi/gkWMBcSyKOxofDYg\niQqqSlEFrviIqSRWEbug7CpiQrC1qVxS8T+2AeOThTkOccWFg3OBHKAMVzAQcCFA0hpZQomwZYOF\nkS0kgZCEfn7vj+5lekezO63dmend6c+rqmuefubZ7udBy7ef/fYvRQRmZlYOY4rugJmZdY6DvplZ\niTjom5mViIO+mVmJOOibmZWIg76ZWYnkDvqSxkhaI2llg+/mS9qZfr9G0q2t7aaZmbXCuKNoeyOw\nAThugO+fjYgrh98lMzNrl1wzfUkzgEXADwZr1pIemZlZ2+RN73wbuAkY7PbdiyX1SnpU0uzhd83M\nzFqtadCXdAWwNSJ6SWbzjWb0q4GZETEH+C7wcEt7aWZmLaFmz96R9M/A54GDwCTgWOCHEXHtID+z\nGbgoIrbX1ftBP2ZmQxARLUmhN53pR8QtETEzIs4ArgGeqg/4kqZlynNJDibbaSA+8xkioiuXpUuX\nFt4Hj8/jK9vYyjC+Vjqaq3f6kbQEiIhYDlwl6XrgALAXuHrAH9y3b6i7NDOzYTqqoB8RzwDPpOXv\nZ+rvBO7MtZH9+49ml2Zm1kKdvyO3i4N+pVIpugtt5fGNXt08Nuj+8bVS0xO5Ld2ZFPGRj8CGDR3b\np5nZaCeJ6NSJ3Jbr4pm+mdlI56BvZlYinQ/6vnrHzKwwnumbmZWIg76ZWYk4vWNmViKdD/qHDiWL\nmZl1XDGvSzxwoJDdmpmVXTFB3ykeM7NCFBP0fTLXzKwQDvpmZiXi9I6ZWYl4pm9mViIO+mZmJZI7\n6EsaI2mNpJUDfH+7pE2SeiXNGXRjTu+YmRXiaGb6NwINH4QvaSFwZkScDSwBlg26Jc/0zcwKkSvo\nS5oBLAJ+MECTxcAKgIhYBUzJviz9CA76ZmaFyDvT/zZwEzDQa7amA29k1rekdY05vWNmVoimL0aX\ndAWwNSJ6JVWAYb2yqwfg3nvhhReoVCp+t6WZWZ1qtUq1Wm3Ltpu+I1fSPwOfBw4Ck4BjgR9GxLWZ\nNsuApyPigXR9IzA/IrbWbSvZ24MPwlVXtXIcZmZdq6PvyI2IWyJiZkScAVwDPJUN+KmVwLVp5+YB\nO+sDfj9O75iZFaJpemcgkpYAERHLI+IxSYskvQbsBq4b9Id9ItfMrBBHFfQj4hngmbT8/brvvpx7\nQw76ZmaF8LN3zMxKxI9hMDMrEQd9M7MScXrHzKxEPNM3MysRB30zsxJxesfMrEQ80zczKxEHfTOz\nEnF6x8ysRDzTNzMrEc/0zcxKxDN9M7MScdA3MysRp3fMzErEM30zsxJpGvQlTZS0StJaSeskLW3Q\nZr6knZLWpMutg27UQd/MrBBN35wVEfskfToi9kgaCzwv6fGIeLGu6bMRcWWuvTq9Y2ZWiFzpnYjY\nkxYnkhwookGz/G9q90zfzKwQuYK+pDGS1gJvAU9GxEsNml0sqVfSo5JmD7pBB30zs0LkejF6RBwG\nLpB0HPCwpNkRsSHTZDUwM00BLQQeBs5ptK0egB07oKeHSqVCpVIZTv/NzLpOtVqlWq22ZduKaJSp\nGeQHpK8DuyPiW4O02QxcFBHb6+qTvR1zDOzaNYTumpmVjyQiIn8KfRB5rt6ZKmlKWp4EXApsrGsz\nLVOeS3Iw6Rfw+3F6x8ysEHnSO6cB90oaQ3KQeCAiHpO0BIiIWA5cJel64ACwF7h60C3u3w8RoJYc\nuMzMLKejTu8Ma2d96R1ILtucMKFj+zYzG606mt5pG6d4zMw6rrig7xu0zMw6zjN9M7MScdA3MysR\np3fMzErEM30zsxJx0DczKxGnd8zMSsQzfTOzEnHQNzMrEad3zMxKxDN9M7MScdA3MysRp3fMzErE\nM30zsxJx0DczK5E8r0ucKGmVpLWS1klaOkC72yVtktQraU7TPTu9Y2bWcU1flxgR+yR9OiL2SBoL\nPC/p8Yh4sa+NpIXAmRFxtqRPAMuAeYNu2DN9M7OOy5XeiYg9aXEiyYGi/h2Li4EVadtVwJTsy9Ib\nctA3M+u4XEFf0hhJa4G3gCcj4qW6JtOBNzLrW9K6gTm9Y2bWcU3TOwARcRi4QNJxwMOSZkfEhqHs\nsKev8PTTVKpVKpXKUDZjZta1qtUq1Wq1LdtWRH2mpskPSF8HdkfEtzJ1y4CnI+KBdH0jMD8ittb9\nbG1vN9wAt902nL6bmZWCJCJCrdhWnqt3pkqakpYnAZcCG+uarQSuTdvMA3bWB/wjOL1jZtZxedI7\npwH3ShpDcpB4ICIek7QEiIhYnq4vkvQasBu4rulWfSLXzKzj8lyyuQ64sEH99+vWv3xUe/ZM38ys\n43xHrplZiTjom5mViJ+yaWZWIp7pm5mViIO+mVmJOL1jZlYinumbmZWIg76ZWYk4vWNmViKe6ZuZ\nlYiDvplZiTi9Y2ZWIp7pm5mViIO+mVmJdD7oj0l3efgwHDzY8d2bmZVZnjdnzZD0lKT1ktZJuqFB\nm/mSdkpaky63DrjBCRNqZc/2zcw6Ks+bsw4CX42IXknHAKslPRER9a9MfDYirmy6tQkT4P33k/L+\n/TB58lF22czMhqrpTD8i3oqI3rT8HvAqML1B03wv7Z04sVb2FTxmZh11VDl9SacDc4BVDb6+WFKv\npEclzR5wI07vmJkVJk96B4A0tfMQcGM6489aDcyMiD2SFgIPA+c03JCDvplZYXIFfUnjSAL+fRHx\nSP332YNARDwu6XuSToyI7fVte95994Ny5d//ncqZZw6p42Zm3aparVKtVtuybUVE80bSCmBbRHx1\ngO+nRcTWtDwX+D8RcXqDdhHnnw+vvJJUrF0Lc+YMvfdmZiUgiYjId960iaYzfUmXAH8OrJO0Fgjg\nFmAWEBGxHLhK0vXAAWAvcPWAG3R6x8ysME2DfkQ8D4xt0uZO4M5ce/TVO2Zmhen8Hbme6ZuZFcZB\n38ysRDof9J3eMTMrjGf6ZmYl4qBvZlYiTu+YmZWIZ/pmZiXioG9mViJO75iZlYhn+mZmJeKgb2ZW\nIk7vmJmViGf6ZmYlUmzQ90zfzKyjik3veKZvZtZRTu+YmZVI06AvaYakpyStl7RO0g0DtLtd0iZJ\nvZIGfgei0ztmZoXJ82L0g8BXI6JX0jHAaklPRMTGvgaSFgJnRsTZkj4BLAPmNdya0ztmZoVpOtOP\niLciojctvwe8Ckyva7YYWJG2WQVMkTSt4Qad3jEzK8xR5fQlnQ7MAVbVfTUdeCOzvoUjDwwJp3fM\nzAqTO+inqZ2HgBvTGf/QOL1jZlaYPDl9JI0jCfj3RcQjDZpsAT6cWZ+R1h2hZ8WKD8qVbduo5O2p\nmVlJVKtVqtVqW7atiGjeSFoBbIuIrw7w/SLgSxFxhaR5wHci4ogTuZIiXn4Z/viPk4o5c2Dt2uH0\n38ys60kiItSKbTWd6Uu6BPhzYJ2ktUAAtwCzgIiI5RHxmKRFkl4DdgPXDbhBp3fMzArTNOhHxPPA\n2Bztvpxrj756x8ysMH72jplZifjZO2ZmJeJn75iZlYjTO2ZmJeL0jplZiXQ+6I8fXysfPAiHD3e8\nC2ZmZdX5oC/1D/ye7ZuZdUzngz7AscfWytu2FdIFM7MyKibon3turbxhQyFdMDMro2KC/nnn1crr\n1xfSBTOzMiom6M+eXSs76JuZdUzxM32nd8zMOqb4oL9+PeR4vLOZmQ1fMUH/tNPg+OOT8rvvwpaG\n71sxM7MWKyboS/3z+k7xmJl1RDFBH3wFj5lZAZoGfUl3S9oq6ZUBvp8vaaekNelya649O+ibmXVc\nnhej3wPcAawYpM2zEXHlUe3Z6R0zs45rOtOPiOeAHU2aHf0Le30Fj5lZx7Uqp3+xpF5Jj0qa3bw5\nvoLHzKwAedI7zawGZkbEHkkLgYeBcwZq3NPT80G5Mn06lZ07k5UNG2DGjBZ0x8xsdKtWq1Sr1bZs\nW5EjrSJpFvCjiDg/R9vNwEURsb3Bd9Fvf1/8Itx1V1L+1rfgK1/J3XEzs7KQREQcfRq9gbzpHTFA\n3l7StEx5LsmB5IiA35Cv4DEz66im6R1J9wMV4CRJrwNLgQlARMRy4CpJ1wMHgL3A1bn37mfwmJl1\nVK70Tst2Vp/eefNNmD49KR93HOzcmdyta2ZmHygivdMevoLHzKyjig36klM8ZmYdVGzQh/5Bf/Xq\n4vphZlYCxQf9efNq5eefL64fZmYlUHzQv+SSWvknP4HDh4vri5lZlys+6J99Npx8clLesQM2biy2\nP2ZmXaz4oC/1n+0/91xxfTEz63LFB33oH/Sd1zczaxsHfTOzEin2jtw++/bBlCnJJ8BvfgOnntqx\nfpmZjWTdc0dun4kT4eMfr617tm9m1hYjI+gDfPKTtbKDvplZW4ycoO+8vplZ242MnD7A9u1w0klJ\nedw4eOcdmDy5Y30zMxupui+nD3DiifCRjyTlgwfhxReL7Y+ZWRcaOUEfnOIxM2uzpkFf0t2Stkp6\nZZA2t0vaJKlX0pwh9yYb9J95ZsibMTOzxvLM9O8BLh/oS0kLgTMj4mxgCbBsyL359Kdr5WeegV27\nhrwpMzM7UtOgHxHPATsGabIYWJG2XQVMyb4s/ajMmgUf+1hS3r8ffvzjIW3GzMwaa0VOfzrwRmZ9\nS1o3NFdeWSuvXDnkzZiZ2ZHGdXqHPT09H5QrlQqVSqV/g8WL4R//MSk/+mhyJc+4jnfTzKww1WqV\narXalm3nuk5f0izgRxFxfoPvlgFPR8QD6fpGYH5EbG3QduDr9PtEwIwZ8OabyXq1CvPnN+2jmVm3\nKuI6faVLIyuBa9OOzQN2Ngr4uUlO8ZiZtUmeSzbvB34CnCPpdUnXSVoi6YsAEfEYsFnSa8D3gb8Z\ndq8WL66VH3kkmf2bmdmwjZzHMGTt2wdTp8J77yXr69fD7Nnt7ZyZ2QjVnY9hyJo4ES7P3BrgFI+Z\nWUuMzKAPR6Z4zMxs2EZmegfg7bdh2jQ4dCg5ufuLX8Dpp7e1f2ZmI1H3p3cgeczyZz+blCPge98r\ntj9mZl1g5M70Af7t3+BP/zQpn3AC/PrXfsa+mZVOOWb6AAsXwhlnJOUdO+D++4vtj5nZKDeyg/7Y\nsfA3mcv+77jD1+ybmQ3DyE7vQDLDnzED9uxJ1p99Fj71qdZ3zsxshCpPegeSXP7nP19bv+OO4vpi\nZjbKjfyZPsC6dXB++qy3sWPhl79MZv9mZiVQrpk+wEc/WnvS5qFD8I1vFNsfM7NRanTM9CF5i9aC\nBbX1557r/05dM7MuVb6ZPiTP4vmzP6utX3998oIVMzPLbfQEfYDvfKd2c9a6dT6pa2Z2lEZX0P/w\nh2Hp0tr6N74BW7YU1x8zs1Fm9OT0++zfDxdcABs2JOuXXZY8rmH8+OF30MxsBOp4Tl/SAkkbJf2n\npK81+H6+pJ2S1qTLra3oXEMTJvR/+NoTTyR37fpOXTOzpvK8LnEM8F3gcuA84HOSzm3Q9NmIuDBd\n/luL+9nf/Pn9L9v8wQ/gn/6prbs0M+sGeWb6c4FNEfGriDgA/CuwuEG7lvzpkVtPD1x7bW3961+H\ne+/taBfMzEabPEF/OvBGZv3XaV29iyX1SnpUUvtfaCvBXXfVnrkP8IUvwD/8Axw+3Pbdm5mNRuNa\ntJ3VwMyI2CNpIfAwcE6jhj09PR+UK5UKlUpl6HudMAEeeih5ANu6dX07gJdfhvvug+OPH/q2zcwK\nUq1WqVarbdl206t3JM0DeiJiQbp+MxAR8c1BfmYzcFFEbK+rH/7VO41s2wZXXw1PPVWrO/NMuP32\n5Jn86mzmycyslTp99c5LwFmSZkmaAFwDrKzr0LRMeS7JwWQ7nTJ1avKYhptuqtX9/OdwxRVw6aWw\ndm3HumJmNpLluk5f0gLgNpKDxN0R8S+SlpDM+JdL+hJwPXAA2At8JSJWNdhOe2b6WQ8+CH/5l7Br\nV3bHyWMc/vqvk9cv+pp+MxtFWjnTH303Z+WxdWuS27/rruSpnFmnnJKkghYtSi79nDSp/f0xMxsG\nB/28Xn0Vbr4ZVq5s/P2kSfAnfwIXXwzz5sHcuclLW8zMRhAH/aO1eTPcfTfccw+8+ebgbWfOhD/6\nIzjvPDj3XDjrLDj7bDj1VJ8QNrNCOOgP1cGDUK3CY4/B44/Dxo35f3bSpOSAMGtWskyfDr//+8nn\naaclB4WTT4ZxrboK1sws4aDfKps3w/PPwwsvwKpV8NOfwoEDQ9/emDHJlUSnnJIsJ5+crPctJ50E\nJ55Y+zzhBJgyJfk5M7MBOOi3y4EDsGkT/OxnyfLaa8myaRPs3NmefUpJ4O87ABx/fPLZtxx3XG05\n9tgjl2OOSZYPfch/ZZh1KQf9IrzzDvzqV8ny+uvJuYEtW5LlrbeSZdu2Yvs4cWIS/LPL5MlHLpMm\n1T77lt/7vf7l+mXixGTJlseP93kOsw5w0B+pDhyA3/0Ofvvb2ue2bbXl7bdh+/Zkeftt2LGj//0E\no42UBP8JEwb+zC7jx/cv9603Ko8fn/zlkl3P1o0bd2Q5zzJ2bP9y39K3PmaMD2Q24jjod5ODB5PU\n0TvvJJ995eyya1eyvPturbxrF7z3HuzenXzu2uV3CrRK9mCQPSDUL2PGDF7XrJz9zC7ZukbfN1rq\n20lHtmlW11eub1dfn+czb3mgn220PlBdo6W+HQyvXf2Sp12jNgPVNeGgb0eKgPffrx0E9uxJyrt3\nJ+U9e2Dv3mR9797Gy/vv9//cty8pv/9+//L+/cm6X0xv1jqNDg4/+xn84R+2NOj7zF+3kGo5+alT\nO7PPQ4dqB4C+Zf/+2rJvX5Lyqi/v35+U+9YblfuWgwePXO+ry5YPHerf/tCh2vd95Wx99jNb9qTE\nihLRkd8/B30burFjaweabhHR/2BQf3CoXw4fHni9r1zfpu/gki0fPty4fd93jdo12ka23Pd99uea\ntcvWZdvVb6O+XaP6RuXsenZp9N1gbbMBslm7vn/XgZa87bL7HEqb+ro82nB+yekdM7MiDXRwgOR8\nkuT0jplZ1ziKE7qt4FtBzcxKxEHfzKxEcgV9SQskbZT0n5K+NkCb2yVtSl+OPqe13TQzs1ZoGvQl\njQG+C1wOnAd8TtK5dW0WAmdGxNnAEmBZG/o64rXrRcYjhcc3enXz2KD7x9dKeWb6c4FNEfGriDgA\n/CuwuK7NYmAFQPqaxCnZ9+aWRbf/4nl8o1c3jw26f3ytlCfoTwfeyKz/Oq0brM2WBm3MzKxgPpFr\nZlYiTW/OkjQP6ImIBen6zUBExDczbZYBT0fEA+n6RmB+RGyt25bvzDIzG4JO3pz1EnCWpFnAb4Br\ngM/VtVkJfAl4ID1I7KwP+NC6TpuZ2dA0DfoRcUjSl4EnSNJBd0fEq5KWJF/H8oh4TNIiSa8Bu4Hr\n2tttMzMbio4+e8fMzIrVsRO5eW7wGmkk3S1pq6RXMnUnSHpC0n9I+rGkKZnv/i69Qe1VSZdl6i+U\n9Eo69u90ehwDkTRD0lOS1ktaJ+mGtL4rxihpoqRVktam41ua1nfF+CC5j0bSGkkr0/VuGtsvJf00\n/fd7Ma3rpvFNkfRg2t/1kj7RkfFFRNsXkoPLa8AsYDzQC5zbiX0Ps9+fBOYAr2Tqvgn8bVr+GvAv\naXk2sJYkZXZ6Ot6+v6RWAR9Py48Blxc9trQvpwJz0vIxwH8A53bZGCenn2OBF0juO+mm8X0F+F/A\nyi78/fwFcEJdXTeN738C16XlccCUToyvU4ObBzyeWb8Z+FrR/9Fz9n0W/YP+RmBaWj4V2NhoTMDj\nwCfSNhsy9dcA/73ocQ0w1oeBz3bjGIHJwMvAx7tlfMAM4EmgQi3od8XY0r5sBk6qq+uK8QHHAT9v\nUN/28XUqvZPnBq/R4pRIr0yKiLeAU9L6gW5Qm04y3j4jcuySTif5q+YFkl+6rhhjmv5YC7wFPBkR\nL9E94/s2cBOQPTHXLWODZFxPSnpJ0l+ldd0yvj8Atkm6J03PLZc0mQ6MzzdnDd+oPxMu6RjgIeDG\niHiPI8c0ascYEYcj4gKSWfFcSefRBeOTdAWwNSJ6gcEuhR51Y8u4JCIuBBYBX5L0Kbrg3y41DrgQ\nuDMd426S2Xzbx9epoL8FmJlZn5HWjUZblT5XSNKpwG/T+i3AhzPt+sY4UP2IIGkcScC/LyIeSau7\naowAEfEuUAUW0B3juwS4UtIvgP8N/BdJ9wFvdcHYAIiI36SfvyNJPc6lO/7tIJmRvxERL6fr/5fk\nIND28XUq6H9wg5ekCSR5p5Ud2vdwif4zqZXAF9LyXwCPZOqvkTRB0h8AZwEvpn+ivSNpriQB12Z+\nZiT4HyQ5wdsydV0xRklT+65+kDQJuBR4lS4YX0TcEhEzI+IMkv+fnoqI/wr8iFE+NgBJk9O/QJH0\nIeAyYB1d8G8HkKZw3pB0Tlr1GWA9nRhfB09cLCC5OmQTcHPRJ1Jy9vl+4E1gH/A6yU1nJwD/Lx3L\nE8DxmfZ/R3JW/VXgskz9RSS/sJuA24oeV6ZflwCHSK6mWgusSf+dTuyGMQIfTcfUC7wC/H1a3xXj\ny/RtPrUTuV0xNpKcd9/v5bq+mNEt40v79TGSCXEv8EOSq3faPj7fnGVmViI+kWtmViIO+mZmJeKg\nb2ZWIg76ZmYl4qBvZlYiDvpmZiXioG9mViIO+mZmJfL/Aa9QpftC5tJyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1270eca10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x_axis = np.arange(0, 6000, 60)\n",
    "\n",
    "plt.plot(x_axis, y_error,'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
